
###################################################
#TODOs
#  - (Dave) Check GAMs convergence information to check that poor GAM with interaction performance is not caused by misspecified convergence parameters etc.
#	- scatter plot of TSS/kappa/ROC vs. RMSE + linear fit -> which performance measures represents underlying probability fits most realistically
#	- decide on absence data:
#		- repeating subsampling
#		- prevalence=0.5
#	- decide on how many repeated drawing of presence data (currently at 40 realizations)
#	- decide on how many evaluation splits (currently at 10 calibrations/evaluations) 
#   - decide how to measure uncertainty (currently standard deviations)
#	- maps:
#		- (Dave) raster maps
#		- decide on spatial resolution of projected maps (1/32-degrees vs 800 m)
#		- figures with all four regions as panels of one file?
#Done
#	- Error check new subsampling protocols
#	- Rewrite data sampling and biomod calls to enforce proper subsampling
#	- model formulae should use centered variables for squared and interaction terms
#	- parallelize model building
#	- evaluate model values with 'true' probabilities: RMSE, deviance
#	- figures: response curves with Elith's lines
#	- center variables with mean from basis region (here, region 2) for variables from all regions
#	- We are not concerned about Warnings: glm.fit: fitted probabilities numerically 0 or 1 occurred
#	- Repeated splitting of data into calibration and validation outside of biomod2
#	- Repeated drawing of presence data from underlying probabilities (realizations)
#		--> data from files is read only once and presence realizations made: all functions draw from this R object instead of repeated reading from files
#		--> SAMP object moved from 'do.SDMs' section to inside function 'make.biomod2sSDM' otherwise object size would become quite large
#	- Added option 'do.ExampleForDropbox': if set to TRUE, then output saved to separate folders on dropbox to serve as examples during code development
#	- Added figures showing scatter between predicted and true probabilities
#	- Code is more verbose and checks for successful completion of SDMs, evaluations, and figures
#	- Temporary evaluation arrays are organized by realization and data-split replications
#	- Fixed important bug in evalX.SDMs:
#		(i) biomod2-evaluations were calculated based on incorrect presence-data realization
#		(ii) rmse and mae evaluations were based on presence-data instead of underlying probabilities of occurrence
#	- Parallelization cannot handle our bres object with 40 realizations x 50 data-split repeats (ca. 13 GB in memory): export to slaves is done via un/serialize with long vectors if needed, but long vectors are not (yet) supported by Rmpi
#		-> DOESN'T WORK: using snow with foreach
#		-> works: added direct Rmpi master-slave communication with function work() which allows only passing a subset of bres that is actually required for one evaluation/figure set (subsets are ca. 0.7-1.8 GB in memory)
#   - (Dave) Rewrite model fitting code to use glm and gam functions directly
#	- (Dave) reduce memory needs/storage (roughly 90% reduction in model and projection memory)
#		--> (done) store predictions as integers 
#		--> (done) minimize model storage (after predictions are made, we do not really need most of the model output)
#	- (Dave) Do the figures "Fig_TrueVsPredictedProbs_*.pdf" generated by plot_scatterPredvsTrueProbs() make sense?
#		-> they are rather skewed to (low obs/high pred) values
#		-> This was a problem with biomod that was corrected by modeling SDMs directly with glm and gam functions
#	- (Daniel) Update response curve functions to account for new data structure; predictions happen now only in the make.sdm() function
#	- (Daniel) Scale maps uniformly for easy comparison
#	- (Daniel) Draw 68% and 90% of model response curves instead of min-median-max
#	- (Daniel) Make plot with differences of performance measures to base region, plus base region values
#	- (Dave) Partition variation from realization vs data-split repeats -- Use Sum of Squares from aov function
###################################################

## Actions
do.ExampleForDropbox <- FALSE #set this to TRUE to access/write to 'Example' subset on dropbox
do.SDMs <- TRUE
do.Partition <- TRUE
do.Evaluation <- TRUE
do.EvaluationSummary <- TRUE
do.Figures <- TRUE

##
## Biomod2
libraries <- c("reshape2", "mgcv", "seqinr", "raster","parallel","mvtnorm")  ##"biomod2", ##
temp <- lapply(libraries, FUN=require, character.only=TRUE)

date.run <- "20160209" #label for output folders (20140228; 20140304; 20140314; 20140320; 20140321; 20140627; 20150130)

## Directories
dir.prj <- "E:/Work_Share_Dave_SDM&ResponseCurves"
path.functions <-"E:/Dropbox/Work_Share_Dave_SDM&ResponseCurves/BIOMOD2/SDMandResponseCurveShapes_Functions.R" 
#dir.prj <- "/Users/drschlaep/Dropbox/Work_Share_Dave_SDM&ResponseCurves"
dir.loc <- dir.prj
#dir.loc <- "/Users/drschlaep/Documents/drschlaepfer/2_Research/200907_UofWyoming_PostDoc/Projects_My/Product21_SDM_AsymmetryResponseCurve/2_Data"
dir.in <- file.path(dir.prj, "data", "csv.files")
dir.gis <- file.path(dir.prj, "data", "rasters")
dir.sdm <- file.path(ifelse(do.ExampleForDropbox, dir.prj, dir.loc), ifelse(do.ExampleForDropbox, "dataExample", "data"), ifelse(do.ExampleForDropbox, "biomodSDMsExample", paste("biomodSDMs",date.run,sep="_")))
dir.dat <- file.path(ifelse(do.ExampleForDropbox, dir.prj, dir.loc), ifelse(do.ExampleForDropbox, "dataExample", "data"))
dir.out <- file.path(dir.prj, ifelse(do.ExampleForDropbox, "FiguresExample", "Figures"), "SDMs")
dir.maps <- file.path(dir.out, paste0("Maps_", date.run))
dir.figs <- file.path(dir.out, paste0("Plots_", date.run))
dir.tables <- file.path(dir.out, paste0("Tables_", date.run))
temp <- lapply(c(dir.sdm, dir.dat, dir.maps, dir.figs, dir.tables), FUN=dir.create, showWarnings=FALSE, recursive=TRUE)

## Settings
num_cores <- 4
parallel_backend <- "snow" # "snow" (here, using sockets; don't use with too many SDMs) or "mpi" (requiring a MPI installed)
filename.saveSDMs <- ifelse(do.ExampleForDropbox, "SDMs_vExample.RData", paste0("SDMs_v2_", date.run, ".RData"))
filename.saveEvals <- ifelse(do.ExampleForDropbox, "SDMs_Evaluations_vExample.RData", paste("SDMs_Evaluations_v2","_",date.run,".RData",sep=""))
filename.saveParts <- ifelse(do.ExampleForDropbox, "SDMs_VarPartition_vExample.RData", paste("SDMs_VarPartition_v2","_",date.run,".RData",sep=""))
filename.saveTypeData <- paste0("TypeData_", date.run, ".RData")
baseRegion <- 2 #NorthWest
regions <- 1:4
types <- c("AIF", "SCT", "SIF", "SIT")
mlevels <- list(woInt=c("linear", "squared"), wInt=c("linear", "squared", "interaction"))
sdm.models <- c("GLM", "GAM")
eval.methods <- c('TSS','ROC','KAPPA')
errors <- c("binom","binom+res","spatial","spatial+res")
biomod2.NbRunEval <- switch(EXPR=paste0("v_", date.run),
                            v_20140228=50,
                            v_20140304=10,
                            v_20140314=10,
                            v_20140320=2,
                            v_20140321=50,
                            v_20140627=25,
                            v_20150130=25,
                            v_20160209=4)
presenceRealizationsN <- switch(EXPR=paste0("v_", date.run),
                                v_20140228=40,
                                v_20140304=5,
                                v_20140314=40,
                                v_20140320=2,
                                v_20140321=40,
                                v_20140627=25,
                                v_20150130=25,
                                v_20160209=4)
predictorsN <- 10
equalSamples <- FALSE #if TRUE, ensures that subsamples have the same number of presences as absences
keepPart <- FALSE #if TRUE, keep all the evaluation numbers for each site as part of the variance partitioning


## Define set of biomod runs
runRequests <- expand.grid(sdm.models, names(mlevels), types, errors,1:presenceRealizationsN, 1:biomod2.NbRunEval, stringsAsFactors=FALSE, KEEP.OUT.ATTRS=FALSE)
colnames(runRequests) <- c("models", "mlevels", "types", "errors","realizations", "run")
runRequestsIDs <- apply(runRequests, MARGIN=1, FUN=function(x) paste(trim(x), collapse="_"))

if(do.ExampleForDropbox){
  if(file.exists(ftemp <- file.path(dir.dat, "runRequests_vExample.rds"))){
    runRequests <- readRDS(file=ftemp)
  } else {
    #runRequests <- runRequests[sample(x=nrow(runRequests), size=5), ]
    runRequests <- rbind(runRequests[runRequests$models == "GLM" & runRequests$mlevels == "wInt" & runRequests$types=="AIF" & runRequests$run == 1, ][1:5, ],
                         runRequests[runRequests$models == "GAM" & runRequests$mlevels == "woInt" & runRequests$types=="AIF" & runRequests$run == 1, ][1:5, ])
    saveRDS(object=runRequests, file=ftemp)
  }
}

runEvals <- unique(runRequests[, c("models", "mlevels", "types","errors")]) #get the unique combinations of model, type, and mlevel

####### Set up parallel environment
#if(!interactive()){
if(identical(parallel_backend, "mpi")){
  stopifnot(require("Rmpi"))
  libraries <- c(libraries, "Rmpi")
  
  .Last <- function() { #Properly clean up mpi before quitting R (e.g., at a crash)
    if(is.loaded("mpi_initialize") && exists("mpi.comm.size")){
      if (mpi.comm.size(1) > 0) mpi.close.Rslaves()
      .Call("mpi_finalize")
    }
  }
  
  mpi.spawn.Rslaves(nslaves=num_cores)
  
  exportObjects <- function(allObjects) {
    for(obj in 1:length(allObjects)) {
      bcast.tempString <- allObjects[obj]
      bcast.tempValue <- try(eval(as.name(allObjects[obj])))
      if(!inherits(bcast.tempValue, "try-error")){
        mpi.bcast.Robj2slave(bcast.tempString)
        mpi.bcast.Robj2slave(bcast.tempValue)
        mpi.bcast.cmd(cmd=try(assign(bcast.tempString, bcast.tempValue)))
      } else {
        print(paste(obj, bcast.tempString, "not successful"))
      }
    }
  }
  
} else if(identical(parallel_backend, "snow")){
  stopifnot(require("snow"), require("doSNOW"))
  libraries <- c(libraries, "snow", "doSNOW")
  
  cl <- snow::makeSOCKcluster(num_cores)
  registerDoSNOW(cl)
  
  .Last <- function() { #Properly clean up snow before quitting R (e.g., at a crash)
    if(exists("stopCluster") && exists("cl")){
      snow::stopCluster(cl)	#clean up snow cluster
    }
  }
} else {
  stop("No available parallel backend specified.")
}	
#}

####### Functions

source(path.functions)

################
## Read data from files once and generate observations
if(file.exists(ftemp <- file.path(dir.in, filename.saveTypeData))){
  load(ftemp)
} else {
  
  tname <- paste(rep(types,times=length(var.obs)),rep(var.obs,each=length(types)),sep="_")
  tmat <- cbind(type=rep(types,times=length(var.obs)),var=rep(var.obs,each=length(types)))
  
  typeData <- varData <- obsData <- probData <- vector("list", length=length(tname))
  names(typeData) <- names(varData) <- names(obsData) <- names(probData) <- tname
  
  sigma <- logit(.5) - logit(.4) #standard deviation set so that sd = +/- 0.10 when p = 0.5
  
  for(tp in 1:length(tname)){
    typeData[[tname[tp]]] <- get_TypeData_FromFile(type=tmat[tp,"type"], center=TRUE, centerBasedOnRegionIDs=baseRegion)
    varData[[tname[tp]]]  <- tmat[tp,"var"]
    probData[[tname[tp]]] <- typeData[[tname[tp]]]$dat[,'prob']
    
    if(tp == 1) 
      dtmp <- as.matrix(dist(typeData[[tname[tp]]]$dat[,c("x","y")]),nrow = nrow(typeData[[tname[tp]]]$dat[,c("x","y")]))
    
      w <- probData[[tname[tp]]]*0
      for(j in 1:length(w)) {
       
        kk <- which(dtmp[,j] != 0 & dtmp[,j] < 1)
        
        w[j] <- sum(dtmp[kk,j] * probData[[tname[tp]]][kk]) / sum(dtmp[kk,j])
      }

    
    obsData[[tname[tp]]]  <- calc_ObservationsFromProbabilities(probs=probData[[tname[tp]]], 
                                                          N=presenceRealizationsN,
                                                          VAR=varData[[tname[tp]]],
                                                          sigma = sigma,
                                                          w = w)
  }
  
  rm(list = c("dtmp", "kk"))
  
  # Climate Data -- one matrix with x, y, region, and climate data
  climData <- typeData[['AIF_binom']]$dat[,-grep('prob',colnames(typeData[['AIF_binom']]$dat))]
  # centerMeans Data
  centerMeansData <- typeData[['AIF_binom']]$centerMeans
  
  rm(typeData)
  save(probData, obsData, climData, centerMeansData, file=ftemp)
}



## Build SDMs
if(do.SDMs){
  print(paste(Sys.time(), ": SDMs started"))
  
  source(file.path(dir.prj, "BIOMOD2", "ourResponsePlot2.r"), echo=FALSE, keep.source=FALSE)
  
  list.export <- c("libraries", "climData", "obsData", "probData", "centerMeansData", 
                   "runRequests", "baseRegion", "regions", "mlevels", "sdm.models", 
                   "eval.methods", "predictorsN", "make.SDM", "set_Data", "calc_sdms", 
                   "set_options", "make_projection","dir.sdm", "dir.in","equalSamples",
                   "get.balanced.sample","get.cutoff", "our.response.plot2")
  if(identical(parallel_backend, "mpi")){
    exportObjects(list.export)
    mpi.bcast.cmd(lapply(libraries, FUN=require, character.only=TRUE))
    
    bres <- mpi.applyLB(x=1:nrow(runRequests), fun=make.SDM)
    
    mpi.bcast.cmd(rm(list=ls()))
    mpi.bcast.cmd(gc())
  }
  if(identical(parallel_backend, "snow")){
    if(length(xt <- 1:nrow(runRequests)) > 1){
      clusterExport(cl, list.export)
      clusterEvalQ(cl, lapply(libraries, FUN=require, character.only=TRUE))
      
      bres <- parLapply(cl, x=xt, fun=function(i) try(make.SDM(i), silent=TRUE))
      
      clusterEvalQ(cl, rm(list=ls()))
      clusterEvalQ(cl, gc())
    } else {
      bres <- as.list(make.SDM(1))
    }
  }
  
  #Identify runs
  goodRuns <- sapply(bres, FUN=function(l) !inherits(l, "try-error"))
  if(any(!goodRuns)) print(bres[[which(!goodRuns)[1]]])
  if(all(!goodRuns)) stop(paste(Sys.time(), ": No SDM successful"))
  print(paste(Sys.time(), ":", sum(goodRuns), "out of", length(goodRuns), "SDMs successful"))
  bres <- bres[goodRuns]
  temp <- t(sapply(bres, FUN=function(l) l$runID))
  #runIDs = index for which row of runRequests corresponds to elements of bres
  runIDs <- na.exclude(match(apply(temp, 1, paste, collapse="_"), table=apply(runRequests, MARGIN=1, FUN=function(x) paste(trim(x), collapse="_"))))
  
  save(list = c("bres", "runIDs"), file=file.path(dir.dat, filename.saveSDMs))
  print(paste(Sys.time(), ": SDMs done"))
}

if(do.Partition){
  
  print(paste(Sys.time(), ": Partition started"))
  
  if(!exists("bres") || !exists("runIDs")){
    load(file.path(dir.dat, filename.saveSDMs))
  }
  
  stat.methods <- 'Testing.data'
  variables <- c('TSS','KAPPA','ROC','RMSE','MAE')
  factors <- c('types',"errors",'models','mlevels','realizations')
  
  part.region <- list()	
  
  for(ir in regions){
    part.mat <- cbind(runRequests,matrix(NA,nrow=nrow(runRequests),ncol=length(variables)))
    colnames(part.mat) <- c(colnames(runRequests),variables)
    
    #get evaluation statistics
    
    for(i in 1:nrow(runRequests)){
      
      prob <- probData[[paste(runRequests[i,'types'],runRequests[i,'errors'],sep="_")]][climData[,'region'] == ir]
      
      obs  <- obsData[[paste(runRequests[i,'types'],runRequests[i,'errors'],sep="_")]][, runRequests[i,'realizations']][climData[,'region'] == ir]
      
      pred <- bres[[i]]$Proj[[ir]]$Proj$pred/1000
      
      cutoff <- get.cutoff(pred = pred,
                           obs = obs,
                           pred.eval = pred,
                           obs.eval = obs,
                           method = eval.methods)[,stat.methods]
      
      if('TSS' %in% variables) part.mat[i,'TSS'] <- cutoff['TSS']
      if('ROC' %in% variables) part.mat[i,'ROC'] <- cutoff['ROC']
      if('KAPPA' %in% variables) part.mat[i,'KAPPA'] <- cutoff['KAPPA']
      if('RMSE' %in% variables) part.mat[i,'RMSE'] <- rmse(obs=prob, pred=pred)
      if('MAE' %in% variables) part.mat[i,'MAE'] <- mae(obs=prob, pred=pred)
      
    }
    
    #run ANOVAs to estimate partitioning of variation
    
    part.out <- vector(mode="list", length=length(variables))
    names(part.out) <- variables
    
    for(j in 1:length(variables)){
      
      tmp <- part.mat[,c(variables[j],colnames(runRequests))]
      colnames(tmp) <- c('y',colnames(runRequests))
      tmp <- as.data.frame(tmp)
      
      tmp$realizations <- apply(tmp[,factors],1,paste,collapse=".")
      
      fit <- aov(y ~ factor(types)*factor(errors)*factor(models)*factor(mlevels) + Error(realizations),data=tmp) 
      
      fnames <- rownames(summary(fit)[[1]][[1]])
        ftmp <- strsplit(fnames,split = c("\\:"))
        for(ff in 1:(length(ftmp)-1)){
          for(hh in 1:length(ftmp[[ff]])){
            htmp <- strsplit(ftmp[[ff]][hh],split=NULL)[[1]][-(1:6)]
            ftmp[[ff]][hh] <- paste(htmp[htmp != ")" & htmp != "(" & htmp!= " "],collapse="")
          }
          htmp <- strsplit(ftmp[[ff]][hh],split=NULL)[[1]]
          ftmp[[length(ftmp)]] <- paste(htmp[htmp!= " "],collapse="")
        }
      fnames <- c("realizations",unlist(lapply(ftmp,paste, collapse=":")))
      
      prop <- as.data.frame(matrix(NA,nrow=length(fnames),ncol=3))
      colnames(prop) <- c('factor','SS','prop')
      prop[,1] <- fnames
      
      #extract sum of squares
      prop[-1,2] <- summary(fit)[[1]][[1]][,2]
      prop[1,2]  <- summary(fit)[[2]][[1]][,2]
      
      #calculate proportion of variation explained
      prop[,3] <- as.numeric(prop[,2]) / sum(as.numeric(prop[,2]))
      
      part.out[[j]] <- vector(mode="list", length=1)
      names(part.out) <- c('prop')
      
      if(keepPart) {
        part.out[[j]] <- vector(mode="list", length=2)
        names(part.out) <- c('eval','prop')
        part.out[[j]]$eval <- part.mat
        
      }
      
      part.out[[j]]$prop <- prop
      
    }
    
    
    part.region[[ir]]<- part.out
    
  }
  
  
  save(part.region, file=file.path(dir.dat, filename.saveParts))
  
  var.sort <- c(2:(length(fnames)-1),1,length(fnames))
  reg.sort <- c(2,1,3,4)
  
  part.mat <- matrix(NA,nrow=length(reg.sort)*length(variables),ncol=length(var.sort))
    colnames(part.mat) <- c('region','metric',(part.region[[1]][[2]]$prop[var.sort,1]))
    part.mat[,'region'] <- rep(c('NR','SR','SW','GP'),each=5)
    part.mat[,'metric'] <- rep(variables,times=4)
  
  for(j in 1:length(reg.sort))
    for(i in 1:length(variables))
      part.mat[which(part.mat[,'metric'] == variables[i])[j],3:11] <- (part.region[[reg.sort[j]]][[i]]$prop[var.sort,3])
  
  write.csv(part.mat,"var.part.csv",quote=FALSE)
  
  print(paste(Sys.time(), ": Partition done"))
  
}

##Get effective degrees of freedom
edf <- rep(NA,length(bres))

for(ee in 1:length(edf)){
  
  if(runRequests[ee,"models"] == "GAM") edf[ee] <- sum(bres[[ee]]$SDMs$edf)
  
  if(runRequests[ee,"models"] == "GLM" & runRequests[ee,"mlevels"] == "woInt") edf[ee] <- 5
  
  if(runRequests[ee,"models"] == "GLM" & runRequests[ee,"mlevels"] == "wInt") edf[ee] <- 6
  
}

png(paste(dir.out,"DF.png",sep="/"),width=6,height=4,units="in",res=600)
par(mar=c(8,4,1,1))
tmp <- boxplot(edf ~ apply(runRequests[,1:3],1,paste,collapse="_"),axes=FALSE,frame.plot=TRUE)
axis(2)
axis(1,at = 1:16, labels = tmp$names,las=3)
mtext("Degrees of Freedom",side=2,line=2.5)
dev.off()

## Evaluate SDMs
if(do.Evaluation){
  print(paste(Sys.time(), ": Evaluation started"))
  
  if(!exists("bres") || !exists("runIDs")){
    load(file.path(dir.dat, filename.saveSDMs))
  }
  
  list.export <- c("libraries", "obsData","probData","climData","centerMeansData", "baseRegion", 
                   "eval2.SDMs", "regions", "eval.methods", "sdm.models", "rmse", "mae","get.cutoff",
                   "dir.sdm", "dir.in")
  if(identical(parallel_backend, "mpi")){
    exportObjects(c("work", list.export))
    mpi.bcast.cmd(lapply(libraries, FUN=require, character.only=TRUE))
    mpi.bcast.cmd(work())
    
    jobs <- 1:nrow(runEvals)
    beval <- vector("list", length=length(jobs)) #Result container
    workersN <- (mpi.comm.size() - 1)
    junk <- 0
    closed_slaves <- 0
    runs.completed <- 1
    
    while(closed_slaves < workersN) {
      complete <- mpi.recv.Robj(mpi.any.source(), mpi.any.tag())
      complete_info <- mpi.get.sourcetag()
      slave_id <- complete_info[1]
      tag <- complete_info[2]
      
      if(tag == 1) {# slave is ready for a task. Give it the next task, or tell it tasks are done if there are none.
        if(runs.completed <= length(jobs)){# Send a task, and then remove it from the task list
          dataForRun <- list(fun="eval",
                             i=i <- jobs[runs.completed],
                             runEval=runEvals[i, ],
                             type=type <- runEvals[i, "types"],
                             error=error <- runEvals[i, "errors"],
                             mlevel=mlevel <- runEvals[i, "mlevels"],
                             model=model <- runEvals[i, "models"],
                             runID=runID <- with(runRequests[runIDs, ], which(types == type & errors == error & mlevels == mlevel & models == model)),
                             bsub=bres[runID])
          print(paste(Sys.time(), ": Evaluation for", dataForRun$i, "started"))
          mpi.send.Robj(dataForRun, slave_id, 1)
          runs.completed <- runs.completed + 1
        } else {
          mpi.send.Robj(junk, slave_id, 2)
        }
      } else if (tag == 2) { # The message contains results
        beval[[complete$i]] <- complete$r
        print(paste(Sys.time(), ": Evaluation for", complete$i, "ended"))
      } else if (tag == 3) { # A slave has closed down.
        closed_slaves <- closed_slaves + 1
      }		
    }								
    
    mpi.bcast.cmd(rm(list=ls()))
    mpi.bcast.cmd(gc())
  }
  if(identical(parallel_backend, "snow")){
    clusterExport(cl, c("bres", "runRequests", "runEvals", "runIDs", list.export)) #TODO: exporting large bres will fail
    clusterEvalQ(cl, lapply(libraries, FUN=require, character.only=TRUE))
    
    list.noexport <- (temp <- ls())[!(temp %in% list.export)]
    beval <- foreach(i=1:nrow(runEvals), .errorhandling="pass", .noexport=list.noexport) %dopar% {
      type <- runEvals[i, "types"]
      error <- runEvals[i,"errors"]
      mlevel <- runEvals[i, "mlevels"]
      model <- runEvals[i, "models"]
      runID <- with(runRequests[runIDs, ], which(types == type & errors == error & mlevels == mlevel & models == model))
      eval2.SDMs(i, runEval=runEvals[i, ], type=type, error = error, mlevel=mlevel, model=model, runID=runID, bsub=bres[runID])
    }
    
    clusterEvalQ(cl, rm(list=ls()))
    clusterEvalQ(cl, gc())
  }
  
  
  #Identify runs
  goodRuns <- sapply(beval, FUN=function(l) !(inherits(l, c("try-error", "simpleError"))))
  if(any(!goodRuns)) print(beval[[which(!goodRuns)[1]]])
  if(all(!goodRuns)) stop(paste(Sys.time(), ": No evaluation successful"))
  print(paste(Sys.time(), ":", sum(goodRuns), "out of", length(goodRuns), "evaluations successful"))
  beval <- beval[goodRuns]
  temp <- t(sapply(beval, FUN=function(l) l$evalID))
  #evalIDs = index for which row of runEvals corresponds to elements of beval
  evalIDs <- na.exclude(match(apply(temp, 1, paste, collapse="_"), table=apply(runEvals, 1, paste, collapse="_")))
  
  save(beval, evalIDs, file=file.path(dir.dat, filename.saveEvals))
  
  print(paste(Sys.time(), ": Evaluation done"))
}


## Summarize model evaluations
if(do.EvaluationSummary){
  print(paste(Sys.time(), ": Evaluation summary started"))
  
  if(!exists("beval") || !exists("evalIDs")){
    load(file.path(dir.dat, filename.saveEvals))
  }
  
  #Fill evaluation arrays with values from bres
  evalA_SDMs <- array(NA, dim=c(length(types), length(errors), length(mlevels), length(sdm.models), length(eval.methods)+1, 2), dimnames=list(types, names(mlevels), sdm.models, c(eval.methods, "Deviance"), c("mean", "sd")))
  evalA_Proj <- array(NA, dim=c(length(types), length(errors), length(mlevels), length(regions), length(sdm.models), length(eval.methods)+2, 2), dimnames=list(types, names(mlevels), paste0("region", regions), sdm.models, c(eval.methods, "RMSE", "MAE"), c("mean", "sd")))
  evalA_ProjDiffs <- array(NA, dim=c(length(types), length(errors), length(mlevels), length(regions), length(sdm.models), length(eval.methods)+2, 2), dimnames=list(types, names(mlevels), paste0("region", regions), sdm.models, c(eval.methods, "RMSE", "MAE"), c("mean", "sd")))
  
  for(i in evalIDs){
    evalID <- evalIDs[i]
    it <- which(runEvals[i, "types"] == types)
    ie <- which(runEvals[i, "errors"] == errors)
    il <- which(runEvals[i, "mlevels"] == names(mlevels))
    im <- which(runEvals[i, "models"] == sdm.models)
    
    evalA_SDMs[it, ie, il, im, , "mean"] <- as.numeric(c(t(beval[[evalID]]$Eval$mean[, "Testing.data"]), beval[[evalID]]$Deviance["mean"]))
    evalA_SDMs[it, ie, il, im, , "sd"] <- as.numeric(c(t(beval[[evalID]]$Eval$sd[, "Testing.data"]), beval[[evalID]]$Deviance["sd"]))
    for(ir in seq_along(regions)){
      evalA_Proj[it, ie, il, ir, im, , "mean"] <- as.numeric(c(t(beval[[evalID]]$Proj[[ir]]$Eval$mean[, "Testing.data"]), beval[[evalID]]$Proj[[ir]]$EvalProb$mean))
      evalA_Proj[it, ie, il, ir, im, , "sd"] <- as.numeric(c(t(beval[[evalID]]$Proj[[ir]]$Eval$sd[, "Testing.data"]), beval[[evalID]]$Proj[[ir]]$EvalProb$sd))
      evalA_ProjDiffs[it, ie, il, ir, im, , "mean"] <- as.numeric(c(t(beval[[evalID]]$Proj[[ir]]$EvalDiffToBase$mean[, "Testing.data"]), beval[[evalID]]$Proj[[ir]]$EvalProbDiffToBase$mean))
      evalA_ProjDiffs[it, ie, il, ir, im, , "sd"] <- as.numeric(c(t(beval[[evalID]]$Proj[[ir]]$EvalDiffToBase$sd[, "Testing.data"]), beval[[evalID]]$Proj[[ir]]$EvalProbDiffToBase$sd))
    }
    
  }
  
  #Reshape arrays into matrices
  evalT_SDMs <- acast(melt(evalA_SDMs), formula=Var1+Var2+Var3~Var4+Var5)
  evalT_Proj <- acast(melt(evalA_Proj), formula=Var1+Var2+Var3+Var4~Var5+Var6)
  evalT_ProjDiffs <- acast(melt(evalA_ProjDiffs), formula=Var1+Var2+Var3+Var4~Var5+Var6)
  
  write.csv(evalT_SDMs, file=file.path(dir.tables, "Table_EvaluationModels.csv"))
  write.csv(evalT_Proj, file=file.path(dir.tables, "Table_EvaluationProjections.csv"))
  write.csv(evalT_ProjDiffs, file=file.path(dir.tables, "Table_EvaluationDifferencesProjections.csv"))
  
  #Dave's figures
  try(source(file.path(dir.prj, "BIOMOD2", "PlotPerformance.r"), echo=FALSE, keep.source=FALSE))
  
  print(paste(Sys.time(), ": Evaluation summary done"))
}


#does a serialized version work
if(FALSE){

	print(paste(Sys.time(), ": Figures started"))

	if(!exists("bres") || !exists("runIDs")){
		load(file.path(dir.dat, filename.saveSDMs))
	}

	for(i in 1:nrow(runEvals)){	
		type <- runEvals[i, "types"]
		error <- runEvals[i, "errors"]
		mlevel <- runEvals[i, "mlevels"]
		model <- runEvals[i, "models"]
		runID <- with(runRequests[runIDs, ], which(types == type & errors == error & mlevels == mlevel & models == model))
		bsub <- bres[runID]
	
		temp <- make2.figures(i,type,error,mlevel,model,runID,bsub)
	}

}

## Create figures
if(do.Figures){
	print(paste(Sys.time(), ": Figures started"))

	if(!exists("bres") || !exists("runIDs")){
		load(file.path(dir.dat, filename.saveSDMs))
	}

	## Read rasters
	grids <- list()
#	for(ir in regions) grids[[ir]] <- get_GeographicRaster_FromFile(ir)

	list.export <- c("libraries", "grids", "regions", "baseRegion","climData","probData","obsData","centerMeansData", 
		"make2.figures", "map_distributions", "plot_scatterPredvsTrueProbs", "plot_responseCurves2",
		"dir.sdm", "dir.in", "dir.maps", "dir.figs")
	if(identical(parallel_backend, "mpi")){
		exportObjects(c("work", list.export))
		mpi.bcast.cmd(lapply(libraries, FUN=require, character.only=TRUE))
		mpi.bcast.cmd(work())

		jobs <- 1:nrow(runEvals)
		res <- 0 #Result container
		workersN <- (mpi.comm.size() - 1)
		junk <- 0
		closed_slaves <- 0
		runs.completed <- 1

		while(closed_slaves < workersN) {
			complete <- mpi.recv.Robj(mpi.any.source(), mpi.any.tag())
			complete_info <- mpi.get.sourcetag()
			slave_id <- complete_info[1]
			tag <- complete_info[2]
			
			if(tag == 1) {# slave is ready for a task. Give it the next task, or tell it tasks are done if there are none.
				if(runs.completed <= length(jobs)){# Send a task, and then remove it from the task list
					dataForRun <- list(fun="figures",
										i=i <- jobs[runs.completed],
										type=type <- runEvals[i, "types"],
										error=error <- runEvals[i, "errors"],
										mlevel=mlevel <- runEvals[i, "mlevels"],
										model=model <- runEvals[i, "models"],
										runID=runID <- with(runRequests[runIDs, ], which(types == type & errors == error& mlevels == mlevel & models == model)),
										bsub=bres[runID])
										
					print(paste(Sys.time(), ": Figures for", dataForRun$i, "started"))
					mpi.send.Robj(dataForRun, slave_id, 1)
					runs.completed <- runs.completed + 1
				} else {
					mpi.send.Robj(junk, slave_id, 2)
				}
			} else if (tag == 2) { # The message contains results
				res <- res + complete$r
				print(paste(Sys.time(), ": Figures for", complete$i, "ended"))
			} else if (tag == 3) { # A slave has closed down.
				closed_slaves <- closed_slaves + 1
			}		
		}								

		mpi.bcast.cmd(rm(list=ls()))
		mpi.bcast.cmd(gc())
	}
	if(identical(parallel_backend, "snow")){
		clusterExport(cl, c("bres", "runRequests", "runEvals", "runIDs", list.export)) #TODO: exporting large bres will fail
		clusterEvalQ(cl, lapply(libraries, FUN=require, character.only=TRUE))
	
		list.noexport <- (temp <- ls())[!(temp %in% list.export)]
		res <- foreach(i=1:nrow(runEvals), .combine="+", .errorhandling="remove", .noexport=list.noexport) %dopar% {
					type <- runEvals[i, "types"]
					error <- runEvals[i, "errors"],
					mlevel <- runEvals[i, "mlevels"]
					model <- runEvals[i, "models"]
					runID <- with(runRequests[runIDs, ], which(types == type & errors == error& mlevels == mlevel & models == model))
					make2.figures(i, type=type, error = error, mlevel=mlevel, model=model, runID=runID, bsub=bres[runID])
				}
	
		clusterEvalQ(cl, rm(list=ls()))
		clusterEvalQ(cl, gc())
	}

	#Identify runs
	if(res == nrow(runEvals)) warning(paste(Sys.time(), ": No figure set completely successful"))
	print(paste(Sys.time(), ":", nrow(runEvals) - res, "out of", nrow(runEvals), "figure sets successful"))

  #plot full map
    #make data, (Obs, Fit, XY, model, fun = mean, maxPred = 1000, figname)
   


plot.type<- 'SCT'
plot.error <- "binom"
plot.var <- 'LnP'

save.resp <- TRUE

if(save.resp){
  jpeg(file.path(dir.figs, paste0('ResponseCurves_',plot.type,'_',plot.error,'_',plot.var,'_','.jpeg')),width=6,height=6,units='in',res=300)
  par(mfrow=c(4,4),mar=c(1,1,0,0))
}

for(re in which(runEvals[,'types'] == plot.type & runEvals[,'errors'] == plot.error)[order(runEvals[runEvals[,'types'] == plot.type & runEvals[,'errors'] == plot.error,'models'],decreasing=TRUE)]){    

    ids <- which(apply(runRequests[,1:3],1,paste,collapse='.') == paste(runEvals[re,1:3],collapse='.'))
    ids <- cbind(ids,runRequests[ids,])
    for(ir in seq_along(regions)){

      Ftmp <- Otmp <- matrix(NA, nrow=length(bres[[ids[1,1]]]$Proj[[ir]]$Proj$pred), ncol=nrow(ids))
      
      Rtmp <- list()
      
      if(ir == 1){
        coordsXY <- climData[(climData[, "region"] == regions[ir]), c("x", "y")]
  
        for(rr in 1:nrow(ids)){
          Ftmp[,rr] <- bres[[ids[rr,1]]]$Proj[[ir]]$Proj$pred
          Otmp[,rr] <- obsData[[paste(ids[rr,'types'],ids[rr,'errors'],sep="_")]][(climData[, "region"] == regions[ir]), ids[rr, 'realizations']]
          Rtmp[[rr]] <- bres[[ids[rr,1]]]$ResponseCurvePreds[[ir]]
          
        }

        Fit <- Ftmp
        Obs <- Otmp
        
      }
        if(ir > 1){
          coordsXY <- rbind(coordsXY,
                            climData[(climData[, "region"] == regions[ir]), c("x", "y")])
          
          for(rr in 1:nrow(ids)){
            Ftmp[, rr] <- bres[[ids[rr,1]]]$Proj[[ir]]$Proj$pred
            Otmp[, rr] <- obsData[[paste(ids[rr,'types'],ids[rr,'errors'],sep="_")]][(climData[, "region"] == regions[ir]), ids[rr, 'realizations']]
            Rtmp[[rr]] <- bres[[ids[rr,1]]]$ResponseCurvePreds[[ir]]
          }
        
          Fit <- rbind(Fit,Ftmp)
          Obs <- rbind(Obs, Otmp)
          
        }
            
      plot_responseCurves_CurvesOnly(newdata = climData[climData[,'region'] == regions[ir],c('LnP','LnPScaled','MinT','MinTScaled')],
                           Prob = probData[[paste(runEvals[re,'types'],runEvals[re,'errors'],sep="_")]][climData[,'region'] == regions[ir]],
                           Obs = Otmp, Fit = Ftmp,respCurvePreds = Rtmp,
                           model = runEvals[re,'models'],
                           maxPred=1000,
                           centerMeans = centerMeansData,
                           env = plot.var)
      print(paste(ir,ids[1,'models'],ids[1,'mlevels']))

      }



map_distributions(Obs = Obs,Fit = Fit,XY = coordsXY,model = "",
                  figname = file.path(dir.maps, paste0("Map_TrueVsPredicted_", 
                                                       ids[1,'types'], "_", 
                                                       ids[1,'errors'], "_", 
                                                       ids[1,'mlevels'], "_", 
                                                       ids[1,'models'],'FULL', ".jpg")))

map_distributions(Obs = Obs,Fit = Fit,XY = coordsXY,model = "",fun=sd,
                  figname = file.path(dir.maps, paste0("Map_UncertaintyVsPredicted_", 
                                                       ids[1,'types'], "_", 
                                                       ids[1,'errors'], "_", 
                                                       ids[1,'mlevels'], "_", 
                                                       ids[1,'models'],'FULL', ".jpg")))


}
dev.off()


  jpeg(file.path(dir.maps, paste0('maps_',plot.type,'_',plot.error,'_','.jpeg')),width=5,height=6.5,units='in',res=600)

plt.mat <- rbind(c(0.06,0.36,0.75,0.95), #a
                 c(0.38,0.68,0.75,0.95), #e
                 c(0.70,1.00,0.75,0.95), #i
                 c(0.06,0.36,0.53,0.73), #b
                 c(0.38,0.68,0.53,0.73), #f
                 c(0.70,1.00,0.53,0.73), #j
                 c(0.06,0.36,0.31,0.51), #c
                 c(0.38,0.68,0.31,0.51), #g
                 c(0.70,1.00,0.31,0.51), #k
                 c(0.06,0.36,0.09,0.29), #d
                 c(0.38,0.68,0.09,0.29), #h
                 c(0.70,1.00,0.09,0.29), #l
                 c(0.09,0.33,0.01,0.02), #pred legend
                 c(0.41,0.65,0.01,0.02), #change legend
                 c(0.73,0.97,0.01,0.02))#sd legend

plot.new()
             
let <- c("(a)","(e)","(i)",
         "(b)","(f)","(j)",
         "(c)","(g)","(k)",
         "(d)","(h)","(l)")
mod <- c("GLM w/o Inter.", "GLM w Inter.", "GAM w/o Inter.", "GAM w Inter.")
runmods <- which(runEvals[,'types'] == plot.type & runEvals[,'errors'] == plot.error)[order(runEvals[runEvals[,'types'] == plot.type & runEvals[,'errors'] == plot.error,'models'],decreasing=TRUE)]

for(re in runmods){    
  
  ids <- which(apply(runRequests[,1:3],1,paste,collapse='.') == paste(runEvals[re,1:3],collapse='.'))
  ids <- cbind(ids,runRequests[ids,])
  for(ir in seq_along(regions)){
    
    Ftmp <- Otmp <- matrix(NA, nrow=length(bres[[ids[1,1]]]$Proj[[ir]]$Proj$pred), ncol=nrow(ids))
    
    Rtmp <- list()
    
    if(ir == 1){
      coordsXY <- climData[(climData[, "region"] == regions[ir]), c("x", "y")]
      
      for(rr in 1:nrow(ids)){
        Ftmp[,rr] <- bres[[ids[rr,1]]]$Proj[[ir]]$Proj$pred
        Otmp[,rr] <- obsData[[paste(ids[rr,'types'],id[rr,"errors"],sep="_")]][(climData[, "region"] == regions[ir]), ids[rr, 'realizations']]
        Rtmp[[rr]] <- bres[[ids[rr,1]]]$ResponseCurvePreds[[ir]]
        
      }
      
      Fit <- Ftmp
      Obs <- Otmp
      
    }
    if(ir > 1){
      coordsXY <- rbind(coordsXY,
                        climData[(climData[, "region"] == regions[ir]), c("x", "y")])
      
      for(rr in 1:nrow(ids)){
        Ftmp[, rr] <- bres[[ids[rr,1]]]$Proj[[ir]]$Proj$pred
        Otmp[, rr] <- obsData[[paste(ids[rr,'types'],id[rr,"errors"],sep="_")]][(climData[, "region"] == regions[ir]), ids[rr, 'realizations']]
        Rtmp[[rr]] <- bres[[ids[rr,1]]]$ResponseCurvePreds[[ir]]
      }
      
      Fit <- rbind(Fit,Ftmp)
      Obs <- rbind(Obs, Otmp)
      
    }
    
      print(paste(ir,ids[1,'models'],ids[1,'mlevels']))
    
  }
  
  #Obs, Fit, XY, model, fun = mean, maxPred = 1000, figname,save=TRUE)
  maxPred <- 1000
  
  zmax <- max(Fit, maxPred)/1000
  
  pred <- apply(Fit/maxPred, MARGIN=1, FUN=function(x) mean(x))
  pred.diff <- apply(Fit/maxPred - Obs, MARGIN=1, FUN=mean)
  pred.sd <- apply(Fit/maxPred, MARGIN=1, FUN=sd)
    
  
  cols.pred  <- rev(terrain.colors(n=maxPred))
  cols.diff <- c(cm.colors(n=maxPred))
  cols.sd   <- rev(c(heat.colors(n=maxPred),"#F2F2F2FF"))
  
  pred.seq  <- seq(0,1,length=maxPred)
  diff.seq <- c(-10,seq(-1,1,length=maxPred))
  sd.seq   <- c(0,seq(0.01,.5,length=maxPred))
  
  par(plt=plt.mat[(which(runmods == re) - 1) * 3 + 1,],new = TRUE)

  plot(coordsXY, pch=15, cex=0.7, col="black", asp=1, xlab="", ylab="", axes=FALSE)
  points(coordsXY, pch=15, cex=0.43, col=cols.pred[findInterval(pred,pred.seq)])
    legend("bottomleft",legend=let[(which(runmods == re) - 1) * 3 + 1],bty="n")

    mtext(mod[which(runmods == re)],side=2,las=3,line=-.25)
    if(re == runmods[1]) mtext("Prediction",side=3,line=.2)

  par(plt=plt.mat[(which(runmods == re) - 1) * 3 + 2,],new = TRUE)

  plot(coordsXY, pch=15, cex=0.7, col="black", asp=1, xlab="", ylab="", axes=FALSE)
  points(coordsXY, pch=15, cex=0.43, col=cols.diff[findInterval(pred.diff,diff.seq)], asp=1, xlab="", ylab="", axes=FALSE)
    legend("bottomleft",legend=let[(which(runmods == re) - 1) * 3 + 2],bty="n")
    if(re == runmods[1]) mtext("Bias",side=3,line=.2)

  par(plt=plt.mat[(which(runmods == re) - 1) * 3 + 3,],new = TRUE)

  plot(coordsXY, pch=15, cex=0.7, col="black", asp=1, xlab="", ylab="", axes=FALSE)
  points(coordsXY, pch=15, cex=0.43, col=cols.sd[findInterval(pred.sd,sd.seq)], asp=1, xlab="", ylab="", axes=FALSE)
    legend("bottomleft",legend=let[(which(runmods == re) - 1) * 3 + 3],bty="n")
    if(re == runmods[1]) mtext("SD",side=3,line=.2)

  
 # axis(side=1)
#  axis(side=2)
#  points(XY, pch=16, cex=ifelse(obs > 0, 0.06 + 0.25*obs, 0), col=col2alpha("black", 0.4))
#  mtext(text=model)
  
  #Legend
#  if(save) par(mar=c(2,0,0.1,0))
#  plot(x=seq(from=0, to=zmax, length=maxPred), y=rep(0, maxPred), xlim=c(0, zmax), col=cols, xlab="Suitability score", axes=FALSE)
#  axis(side=1, pos=-0.1, cex=0.6)
  
 
  
}

  par(plt=plt.mat[13,],new = TRUE)
  image(pred.seq,1:2,matrix(pred.seq[-1],ncol=1),col=cols.pred[-length(cols.pred)],
        ylab="",xlab="",axes=FALSE,frame.plot=TRUE)
  box()
  mtext(c("0.0","0.5","1.0"), side=3, at = c(0,.5,1),line=.2,cex=.8)

  par(plt=plt.mat[14,],new = TRUE)
  image(diff.seq[-1],1:2,matrix(diff.seq[-1],ncol=1),col=cols.diff[-length(cols.diff)],
        ylab="",xlab="",axes=FALSE,frame.plot=TRUE)
  box()
  mtext(c("-1.0","0.0","1.0"), side=3, at = c(-1,0,1),line=.2,cex=.8)

  par(plt=plt.mat[15,],new = TRUE)
  image(sd.seq,1:2,matrix(sd.seq[-1],ncol=1),col=cols.sd[-length(cols.sd)],
        ylab="",xlab="",axes=FALSE,frame.plot=TRUE)
  box()
  mtext(c("0.0","0.5","1.0"), side=3, at = c(0,.5,1),line=.2,cex=.8)


dev.off()

rm(Ftmp,Otmp)

  
#finish
  print(paste(Sys.time(), ": Figures done"))
}

## Clean up parallel backends
print(paste(Sys.time(), ": Clean-up"))

if(identical(parallel_backend, "mpi")){
	#mpi.close.Rslaves(dellog=FALSE)
	mpi.exit()
}
